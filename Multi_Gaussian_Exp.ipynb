{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_Gaussian_Exp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsLp6vLgrktwp315FoXsCS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZichuLiu/NER-pytorch/blob/master/Multi_Gaussian_Exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxqvcHxEbufM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as functional\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.autograd import grad, Variable, backward\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import mnist\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from time import time\n",
        "import os\n",
        "import socket\n",
        "from datetime import datetime\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7hFEdepb3jC",
        "colab_type": "text"
      },
      "source": [
        "Define Discriminator and Generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhRVcNZYcxUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=6):\n",
        "        super(Discriminator, self).__init__()\n",
        "        layers = [nn.Linear(input_size, hidden_size)]\n",
        "        for i in range(num_layers - 2):\n",
        "            layers.extend([nn.Tanh(),\n",
        "                           nn.Linear(hidden_size, hidden_size)])\n",
        "        layers.append(nn.Linear(hidden_size, output_size).cuda())\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.output = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.output(self.net(x))\n",
        "\n",
        "    def get_penalty(self, x_true, x_gen):\n",
        "        x_true = x_true.view_as(x_gen).cuda()\n",
        "        alpha = torch.rand((len(x_true),) + (1,) * (x_true.dim() - 1))\n",
        "        if x_true.is_cuda:\n",
        "            alpha = alpha.cuda(x_true.get_device())\n",
        "        x_penalty = Variable(alpha * x_true + (1 - alpha) * x_gen, requires_grad=True).cuda()\n",
        "        p_penalty = self.forward(x_penalty)\n",
        "        gradients = grad(p_penalty, x_penalty, grad_outputs=torch.ones_like(p_penalty).cuda(\n",
        "            x_true.get_device()) if x_true.is_cuda else torch.ones_like(p_penalty), create_graph=True,\n",
        "                         retain_graph=True, only_inputs=True)[0]\n",
        "        penalty = ((gradients.view(len(x_true), -1).norm(2, 1) - 1) ** 2).mean()\n",
        "\n",
        "        return penalty\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=6):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = [nn.Linear(input_size, hidden_size)]\n",
        "        for i in range(num_layers-2):\n",
        "            layers.extend([nn.Tanh(),\n",
        "                          nn.Linear(hidden_size, hidden_size)])\n",
        "        layers.append(nn.Linear(hidden_size, output_size).cuda())\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuAWe7AadKPi",
        "colab_type": "text"
      },
      "source": [
        "Define Sinkhorn distance and related utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrJQ39PvdPqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sinkhorn_normalized(x, y, epsilon, n, niter,p=1):\n",
        "    Cxy = cost_matrix(x, y, p)\n",
        "    Wxy, pi = sinkhorn_loss(x, y, epsilon, n, niter, C_Matrix=Cxy)\n",
        "    Cxx = cost_matrix(x, x, p)\n",
        "    Wxx, pi_x = sinkhorn_loss(x, x, epsilon, n, niter, C_Matrix=Cxx)\n",
        "    Cyy = cost_matrix(y, y, p)\n",
        "    Wyy, pi_y = sinkhorn_loss(y, y, epsilon, n, niter, C_Matrix=Cyy)\n",
        "    return 2 * Wxy - Wxx - Wyy, (pi, pi_x, pi_y)\n",
        "\n",
        "\n",
        "def mixed_sinkhorn_normalized(x, y, fx, fy, epsilon, n, niter, p=1, nfac=1.0):\n",
        "    if len(x.shape) > 2:\n",
        "        x = x.view(x.shape[0], -1).cuda()\n",
        "        y = y.view(y.shape[0], -1).cuda()\n",
        "    Cxy = cost_matrix(x, y, p) + nfac * RKHS_Norm(fx, fy)\n",
        "    # Cxy = cost_matrix(x, y, p) + cost_matrix(fx, fy, p)\n",
        "    Wxy, pi = sinkhorn_loss(x, y, epsilon, n, niter, C_Matrix=Cxy)\n",
        "    Cxx = cost_matrix(x, x, p) + nfac * RKHS_Norm(fx,fx)\n",
        "    # Cxx = cost_matrix(x, x, p) + cost_matrix(fx, fx, p)\n",
        "    Wxx, pi_x = sinkhorn_loss(x, x, epsilon, n, niter, C_Matrix=Cxx)\n",
        "    Cyy = cost_matrix(y, y, p) + nfac * RKHS_Norm(fy,fy)\n",
        "    # Cyy = cost_matrix(y, y, p) + cost_matrix(fy, fy, p)\n",
        "    Wyy, pi_y = sinkhorn_loss(y, y, epsilon, n, niter, C_Matrix=Cyy)\n",
        "    # Wxx = 0\n",
        "    # Wyy = 0\n",
        "    return 2 * Wxy - Wxx - Wyy, pi\n",
        "\n",
        "\n",
        "def sinkhorn_loss(x, y, epsilon, n, niter, C_Matrix):\n",
        "    \"\"\"\n",
        "    Given two emprical measures with n points each with locations x and y\n",
        "    outputs an approximation of the OT cost with regularization parameter epsilon\n",
        "    niter is the max. number of steps in sinkhorn loop\n",
        "    \"\"\"\n",
        "\n",
        "    # The Sinkhorn algorithm takes as input three variables :\n",
        "\n",
        "    C = C_Matrix  # Wasserstein cost function\n",
        "    # both marginals are fixed with equal weights\n",
        "    mu = Variable(1. / n * torch.cuda.FloatTensor(n).fill_(1), requires_grad=False)\n",
        "    nu = Variable(1. / n * torch.cuda.FloatTensor(n).fill_(1), requires_grad=False)\n",
        "\n",
        "    # Parameters of the Sinkhorn algorithm.\n",
        "    rho = 1  # (.5) **2          # unbalanced transport\n",
        "    tau = -.8  # nesterov-like acceleration\n",
        "    lam = rho / (rho + epsilon)  # Update exponent\n",
        "    thresh = 10 ** (-1)  # stopping criterion\n",
        "\n",
        "    # Elementary operations .....................................................................\n",
        "    def ave(u, u1):\n",
        "        \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\"\n",
        "        return tau * u + (1 - tau) * u1\n",
        "\n",
        "    def M(u, v):\n",
        "        \"Modified cost for logarithmic updates\"\n",
        "        \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
        "        return (-C + u.unsqueeze(1) + v.unsqueeze(0)) / epsilon\n",
        "\n",
        "    def lse(A):\n",
        "        \"log-sum-exp\"\n",
        "        return torch.logsumexp(A, dim=1, keepdim=True)\n",
        "\n",
        "    # Actual Sinkhorn loop ......................................................................\n",
        "    u, v, err = 0. * mu, 0. * nu, 0.\n",
        "    actual_nits = 0  # to check if algorithm terminates because of threshold or max iterations reached\n",
        "\n",
        "    for i in range(niter):\n",
        "        u1 = u  # useful to check the update\n",
        "        u = epsilon * (torch.log(mu) - lse(M(u, v)).squeeze()) + u\n",
        "        v = epsilon * (torch.log(nu) - lse(M(u, v).t()).squeeze()) + v\n",
        "        # accelerated unbalanced iterations\n",
        "        # u = ave( u, lam * ( epsilon * ( torch.log(mu) - lse(M(u,v)).squeeze()   ) + u ) )\n",
        "        # v = ave( v, lam * ( epsilon * ( torch.log(nu) - lse(M(u,v).t()).squeeze() ) + v ) )\n",
        "        err = (u - u1).abs().sum()\n",
        "\n",
        "        actual_nits += 1\n",
        "        if (err.data.tolist() < thresh):\n",
        "            break\n",
        "    U, V = u, v\n",
        "    pi = torch.exp(M(U, V))  # Transport plan pi = diag(a)*K*diag(b)\n",
        "    cost = torch.sum(pi * C)  # Sinkhorn cost\n",
        "\n",
        "    return cost, pi\n",
        "\n",
        "\n",
        "def cost_matrix(x, y, p=1):\n",
        "    \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
        "    x_col = x.unsqueeze(1).cuda()\n",
        "    y_lin = y.unsqueeze(0).cuda()\n",
        "    c = torch.sum((torch.abs(x_col - y_lin)).cuda() ** p, 2)\n",
        "    return c\n",
        "\n",
        "\n",
        "def RBF_Kernel(fx, fy, gamma):\n",
        "    \"Returns the matrix of $exp(-gamma * |x_i-y_j|^2)$.\"\n",
        "    x_col = fx.unsqueeze(1).cuda()\n",
        "    y_lin = fy.unsqueeze(0).cuda()\n",
        "    c = torch.norm(torch.abs(x_col - y_lin), p=1, dim=2)\n",
        "    # c = torch.sum((torch.abs(x_col - y_lin)) ** p, 2)\n",
        "    RBF_K = torch.exp(-gamma * c)\n",
        "    return RBF_K\n",
        "\n",
        "\n",
        "def RKHS_Norm(x, y, gamma=0.5):\n",
        "    Kxy = RBF_Kernel(x, y, gamma)\n",
        "    return 1 + 1 - 2 * Kxy"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEbPwqO4diI6",
        "colab_type": "text"
      },
      "source": [
        "Define real sample generators and noise sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2enlyz7odhgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def x_real_builder(batch_size):\n",
        "    sigma = .01\n",
        "    skel = np.array([\n",
        "        [2.0, 2.0],\n",
        "        [2.0, 1.0],\n",
        "        [2.0, 0.0],\n",
        "        [2.0, -1.0],\n",
        "        [2.0, -2.0],\n",
        "        [1.0, 2.0],\n",
        "        [1.0, 1.0],\n",
        "        [1.0, 0.0],\n",
        "        [1.0, -1.0],\n",
        "        [1.0, -2.0],\n",
        "        [0.0, 2.0],\n",
        "        [0.0, 1.0],\n",
        "        [0.0, 0.0],\n",
        "        [0.0, -1.0],\n",
        "        [0.0, -2.0],\n",
        "        [-1.0, 2.0],\n",
        "        [-1.0, 1.0],\n",
        "        [-1.0, 0.0],\n",
        "        [-1.0, -1.0],\n",
        "        [-1.0, -2.0],\n",
        "        [-2.0, 2.0],\n",
        "        [-2.0, 1.0],\n",
        "        [-2.0, 0.0],\n",
        "        [-2.0, -1.0],\n",
        "        [-2.0, -2.0],\n",
        "    ])\n",
        "    temp = np.tile(skel, (batch_size // 25 + 1, 1))\n",
        "    mus = temp[0:batch_size, :]\n",
        "    m = Normal(torch.FloatTensor([.0]), torch.FloatTensor([sigma]))\n",
        "    samples = m.sample((batch_size, 2))\n",
        "    samples = samples.view((batch_size, 2))\n",
        "    return samples.new(mus) + samples  # * .2\n",
        "\n",
        "\n",
        "def get_noise_sampler():\n",
        "    return lambda m, n: torch.randn(m, n).requires_grad_().cuda()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ6KRdYkeWMC",
        "colab_type": "text"
      },
      "source": [
        "Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNGUNFZoeX65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1fbacb63-cb32-47aa-d7ae-17ae4e3e27a2"
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--exp_name\", type=str, default='Sinkhorn_GAN')\n",
        "parser.add_argument(\"--optim\", type=str, default=\"ema\", help=\"optimization algorithm to use\")\n",
        "parser.add_argument(\"--alpha\", type=float, default=0.2)\n",
        "parser.add_argument(\"--g_lr\", type=float, default=1e-3)\n",
        "parser.add_argument(\"--d_lr\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--weight_decay\", type=float, default=2e-5)\n",
        "parser.add_argument(\"--grad_penalty\", type=float, default=1.0)\n",
        "parser.add_argument(\"--mix_metric_flag\", type=int, default=1)\n",
        "parser.add_argument(\"--nonlinear_OT_flag\", type=int, default=0)\n",
        "parser.add_argument(\"--nonlinear_fac\", type=float, default=1.0)\n",
        "\n",
        "parser.add_argument(\"--num_epochs\", type=int, default=2000000)\n",
        "parser.add_argument(\"--num_workers\", type=int, default=4)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=500)\n",
        "parser.add_argument(\"--print_interval\", type=int, default=1000)\n",
        "parser.add_argument(\"--vis_interval\", type=int, default=1000)\n",
        "\n",
        "parser.add_argument(\"--latent_dim\", type=int, default=64)\n",
        "parser.add_argument(\"--hidden_size\", type=int, default=384)\n",
        "parser.add_argument(\"--num_layers\", type=int, default=6)\n",
        "parser.add_argument(\"--seed\", type=int, default=0)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "print(\"see all args:\", args)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "see all args: Namespace(alpha=0.2, batch_size=500, d_lr=0.0001, exp_name='Sinkhorn_GAN', g_lr=0.001, grad_penalty=1.0, hidden_size=384, latent_dim=64, mix_metric_flag=1, nonlinear_OT_flag=0, nonlinear_fac=1.0, num_epochs=2000000, num_layers=6, num_workers=4, optim='ema', print_interval=1000, seed=0, vis_interval=1000, weight_decay=2e-05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbUb33tdfe_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bcbce51-0a84-4bee-b2fd-13707fdbd176"
      },
      "source": [
        "args.exp_name"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Sinkhorn_GAN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFEuDs3-fFB3",
        "colab_type": "text"
      },
      "source": [
        "main training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Xqk0kNfHhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "4ea73494-3d12-4898-87e6-bc45e4e84630"
      },
      "source": [
        "expname = args.exp_name\n",
        "current_time = datetime.now().strftime('%Y-%m-%d_%H')\n",
        "log_dir = os.path.join('runs', expname + \"_\" + current_time + \"_\" + socket.gethostname(),\n",
        "                           'mix' + str(args.mix_metric_flag) + 'fac' + str(args.nonlinear_fac),\n",
        "                           'nonlinear' + str(args.nonlinear_OT_flag))\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "with open(os.path.join(log_dir, \"args.txt\"), \"w\") as fp:\n",
        "    for arg in vars(args):\n",
        "        fp.write(\"%s:%s \\n\" % (arg, str(getattr(args, arg))))\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.random.manual_seed(args.seed)\n",
        "\n",
        "epsilon = .01\n",
        "\n",
        "g_input_size = args.latent_dim\n",
        "\n",
        "d_minibatch_size = args.batch_size\n",
        "\n",
        "num_epochs = args.num_epochs\n",
        "\n",
        "d_learning_rate = args.d_lr\n",
        "g_learning_rate = args.g_lr\n",
        "\n",
        "noise_data = get_noise_sampler()\n",
        "\n",
        "g_hidden_size = args.hidden_size\n",
        "g_output_size = 2\n",
        "\n",
        "d_input_size = 2  \n",
        "d_hidden_size = args.hidden_size\n",
        "d_output_size = 32\n",
        "\n",
        "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size,\n",
        "              num_layers=args.num_layers).cuda()\n",
        "D = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size,\n",
        "                  num_layers=args.num_layers).cuda()\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr=args.g_lr, weight_decay=args.weight_decay)\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr=args.d_lr, weight_decay=args.weight_decay)\n",
        "\n",
        "z_test = torch.rand((d_minibatch_size, g_input_size)).cuda()\n",
        "z_test = Variable((z_test - 0.5) * 2)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    z = torch.rand((d_minibatch_size, g_input_size)).cuda()\n",
        "    z = Variable((z - 0.5) * 2)\n",
        "    images = x_real_builder(d_minibatch_size).float().cuda()\n",
        "    generated_imgs = G.forward(z)\n",
        "\n",
        "    if args.mix_metric_flag:\n",
        "        nfac = args.nonlinear_fac\n",
        "        D_fake = D.forward(generated_imgs)\n",
        "        D_real = D.forward(images)\n",
        "        D_fake = D_fake.view(D_fake.shape[0], -1)\n",
        "        D_real = D_real.view(D_real.shape[0], -1)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            for param in D.parameters():\n",
        "                param.requires_grad = True\n",
        "            for param in G.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            loss, _ = mixed_sinkhorn_normalized(generated_imgs, images, D_fake, D_real, epsilon, d_minibatch_size,\n",
        "                                                500, nfac=nfac)\n",
        "            d_optimizer.zero_grad()\n",
        "            if args.grad_penalty:\n",
        "                grad_penalty = D.get_penalty(images, generated_imgs)\n",
        "                D_loss = -loss + args.grad_penalty * grad_penalty\n",
        "            else:\n",
        "                D_loss = -loss\n",
        "            D_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "        else:\n",
        "            for param in D.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in G.parameters():\n",
        "                param.requires_grad = True\n",
        "            g_loss, _ = mixed_sinkhorn_normalized(generated_imgs, images, D_fake, D_real, epsilon, d_minibatch_size,\n",
        "                                                  500, nfac=nfac)\n",
        "            g_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "            print(g_loss.data.tolist())\n",
        "\n",
        "    elif args.nonlinear_OT_flag:\n",
        "\n",
        "        D_fake = D.forward(generated_imgs)\n",
        "        D_real = D.forward(images)\n",
        "        D_fake = D_fake.view(D_fake.shape[0], -1)\n",
        "        D_real = D_real.view(D_real.shape[0], -1)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            for param in D.parameters():\n",
        "                param.requires_grad = True\n",
        "            for param in G.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            loss, _ = sinkhorn_normalized(D_fake, D_real, epsilon, d_minibatch_size, 500)\n",
        "            d_optimizer.zero_grad()\n",
        "            if args.grad_penalty:\n",
        "                grad_penalty = D.get_penalty(images, generated_imgs)\n",
        "                D_loss = -loss + args.grad_penalty * grad_penalty\n",
        "            else:\n",
        "                D_loss = -loss\n",
        "            D_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "        else:\n",
        "            for param in D.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in G.parameters():\n",
        "                param.requires_grad = True\n",
        "            g_loss, _ = sinkhorn_normalized(D_fake, D_real, epsilon, d_minibatch_size, 500)\n",
        "            g_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "            print(g_loss.data.tolist())\n",
        "    else:\n",
        "        g_loss, _ = sinkhorn_normalized(generated_imgs, images, epsilon, d_minibatch_size, 500)\n",
        "        g_optimizer.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "        print(g_loss.data.tolist())\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        images = x_real_builder(d_minibatch_size)\n",
        "        fake_data = G.forward(z_test)\n",
        "        fake_data = [item.data.tolist() for item in fake_data]\n",
        "        real_data = [item.data.tolist() for item in images]\n",
        "        X = [-2, -1, 0., 1, 2]\n",
        "        Y = [-2, -1, 0., 1, 2]\n",
        "        fig, axes = plt.subplots(1, 1)\n",
        "        for x in X:\n",
        "            for y in Y:\n",
        "                axes.plot(x, y, 'go')\n",
        "        for item in fake_data:\n",
        "            axes.plot(item[0], item[1], 'b.')\n",
        "        for item in real_data:\n",
        "            axes.plot(item[0], item[1], 'r.')\n",
        "\n",
        "        axes.grid()\n",
        "        fig.savefig(os.path.join(log_dir, \"gauss_iter_%i.jpg\" % epoch))\n",
        "\n",
        "        plt.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.248564720153809\n",
            "5.249998092651367\n",
            "4.9268574714660645\n",
            "4.337074279785156\n",
            "3.2011466026306152\n",
            "1.8678901195526123\n",
            "1.9524468183517456\n",
            "2.383619546890259\n",
            "1.8689770698547363\n",
            "1.702895998954773\n",
            "1.6313105821609497\n",
            "1.6742336750030518\n",
            "1.7272125482559204\n",
            "1.9923789501190186\n",
            "2.014218807220459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-fc1f538d1df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             g_loss, _ = mixed_sinkhorn_normalized(generated_imgs, images, D_fake, D_real, epsilon, d_minibatch_size,\n\u001b[0;32m---> 80\u001b[0;31m                                                   500, nfac=nfac)\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-806d33cdaa71>\u001b[0m in \u001b[0;36mmixed_sinkhorn_normalized\u001b[0;34m(x, y, fx, fy, epsilon, n, niter, p, nfac)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mCxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnfac\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mRKHS_Norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Cxy = cost_matrix(x, y, p) + cost_matrix(fx, fy, p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mWxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msinkhorn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_Matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mCxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnfac\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mRKHS_Norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Cxx = cost_matrix(x, x, p) + cost_matrix(fx, fx, p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-806d33cdaa71>\u001b[0m in \u001b[0;36msinkhorn_loss\u001b[0;34m(x, y, epsilon, n, niter, C_Matrix)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m  \u001b[0;31m# useful to check the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# accelerated unbalanced iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-806d33cdaa71>\u001b[0m in \u001b[0;36mlse\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;34m\"log-sum-exp\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Actual Sinkhorn loop ......................................................................\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}